---
layout: post
title: InfoGAN Review
excerpt: "GAN의 변형 중 하나인 InfoGAN에 대한 논문을 읽고 간단하게 요약 및 정리 해봤습니다."
categories: [GAN]
comments: true
use_math: true
---

# Review: Info GAN

<h6 align="right">강병규</h6>

안녕하세요. GAN의 다양한 논문들을 계속 리뷰해오고 있는데 이번에는 [Info GAN](https://arxiv.org/abs/1606.03657)에 대해서 한번 리뷰해보고자 합니다. 먼저 이 논문에서 주장하는 기존 GAN 문제점들에 대해서 알아본 다음 Info GAN은 이를 어떻게 해결했는지에 대해서 이야기할 것입니다. 들어가기 전에 Info GAN에 대해 간략하게만 설명해보겠습니다. 이 논문에서 주장하는 것은 엔트로피와 mutual information이라는 "정보이론"에서 사용하는 개념을 GAN에 반영해서 비지도학습(unsupervised)으로 데이터의 특징을 표현할 수 있다는 것입니다.

예를 들어 사람들의 얼굴 사진을 찍은 데이터가 있다고 해봅시다. Info GAN을 이 데이터에 대해 학습시킨다면 당연히 기존의 GAN과 마찬가지로 이를 모방하는 샘플들을 만들어낼 수 있을 것입니다. 여기에 Info GAN은 추가로 이 데이터들을 잘 설명할 수 있는 변수들을 비지도학습합니다. 이때 비지도학습이 제대로 되었다면 학습한 변수로 데이터들을 분류(classification)하는 것이 가능해집니다. 즉 얼굴 데이터를 잘 분류할 수 있는 변수들, 얼굴의 표정이나 눈의 색, 안경의 유/무 등을 우리가 지정해주지 않고도 자동으로 학습하는 것이죠.

## 기존 GAN의 문제점

기존의 GAN을 생각해봅시다. 특정 개수의 노이즈가 신경망을 거쳐 어떠한 데이터를 만들어낼 것입니다. 하지만 이러한 일반적인 GAN에는 문제점이 존재합니다. 바로 노이즈의 분포가 꼬여져있다는(entangled) 것인데요. 사진을 보며 알아볼까요?

![entangled](https://user-images.githubusercontent.com/25279765/29562618-015ac86e-8775-11e7-8df1-673c9ab87a46.png)

(출처: 이활석님의 ppt)

왼쪽의 분포를 보면 데이터들이 무질서하게 배치되어 있다는 것을 볼 수가 있습니다. 이러한 분포를 "꼬여있다"고 표현합니다. 데이터들이 이렇게 분포해있다면 어떤 유의미한 정보를 파악하기가 어렵습니다. 일반적인 GAN에서 노이즈들의 분포가 저렇다고 생각하시면 됩니다. 분포가 "꼬여있기" 때문에 노이즈를 연속적으로 변화시킨다고 해서 만들어지는 이미지가 연속적이지는 않게 되는 것이죠.

기존의 GAN에서 노이즈의 분포를 오른쪽처럼 어떠한 의미를 갖도록 하려면 어떻게 해야할까요? 오른쪽의 분포를 보면 MNIST 데이터에서 숫자들을 기준으로 데이터들이 일련의 클러스터를 이루고 있는 것을 볼 수 있습니다. 이러한 분포를 풀려있다(disentagled)라고 하는데, 기존 GAN에서 이러한 방식으로 노이즈의 분포를 표현하고 싶다면 데이터 각각에 label을 달아줘야합니다. 즉 지도학습(supervise learning)으로만 이러한 "풀려 있는" 분포를 만들어 낼 수 있는 것이죠.

## INFO GAN

만약 데이터들을 비지도학습만으로 "풀려 있게" 만들 수 있다면 효율적이겠죠? 데이터로부터 샘플을 만들어내는 과정도 생각해봅시다. 제대로 된 샘플이 나오기 위해서는 데이터에서 어떠한 특징을 학습할 수 밖에 없습니다. 즉 적절하게 학습된 generative model은 특징을 잘 표현할 수 있게 됩니다. 또 반대로 특징을 잘 표현할 수 있다면 샘플을 잘 만들어낼 수 있을 것입니다.

문제는 비지도학습을 어떻게 시키는가입니다. 단순히 100차원의 노이즈에 몇 개의 값을 추가해주는 것으로는 학습이 잘 되지 않습니다. Generator가 노이즈를 사용하는 방식에 어떠한 제한도 없기 때문인데요, 즉 그냥 추가해주기만 한다면 노이즈의 차원이 늘어나는 효과 그 이상도 그 이하도 아닌 것이죠.

Info GAN은 Loss에 추가적인 항을 덧붙이는 방식으로 이러한 문제를 해결했습니다. Info GAN에서는 노이즈를 두 개의 값으로 구분했습니다. 하나는 기존 GAN과 같은 방식으로 사용되는 노이즈 ${z}$와, 또 다른 하나는 latent code라고 부르면서, 데이터의 분포에서 중요한 값들을 표현하는데 사용하는 노이즈, ${c}$입니다. 비지도학습으로 데이터의 분포를 풀어서 표현하겠다는 것은 ${c}$를 비지도학습을 통해 학습시키겠다는 뜻과 같습니다. 하지만 일반적인 GAN에 ${z}$와 ${c}$를 함께 주기만 하는 것은 위에서 말했듯 노이즈가 늘어나는 것에 불과합니다. 자, 기존 GAN에서 Loss를 다시한번 생각해봅시다.

$$\min_{G}\max_{D}{V(D,G)} = \mathbb{E}_{x\sim p_{data}(x)}[logD(x)] + \mathbb{E}_{z\sim noise}[log(1-D(G(z)))]$$

여기에 추가적인 항을 더해주기만 하면 ${c}$를 적절하게 학습시킬 수 있습니다.

이 과정에서 등장하는 것이 "Mutual Information, 상호정보량"입니다. ${X}$와 ${Y}$의 상호정보량은 $I(X, Y)$라고 표현하며, 이는 ${Y}$를 알 때, ${X}$의 불확실성이 얼마나 감소하는가를 나타냅니다. 만약 ${X}$와 ${Y}$가 독립(independent)하다면 어떻게 될까요? ${Y}$와 ${X}$사이의 연관성이 전혀없기 때문에 ${Y}$를 알아봤자 ${X}$에 대한 어떠한 정보도 알 수 없습니다. 즉 둘 사이의 상호정보량 ${I(X, Y)}$는 $0$이 되겠죠.

Info GAN의 Loss는 다음과 같이 정의됩니다.

$$\min_{G}\max_{D}{V_I(D,G)} = V(D, G) - \lambda I(c; G(z, c))$$

${V(D, G)}$는 일반적인 GAN의 Loss입니다. 결국 위의 식은 일반적인 GAN의 Loss에 regularization term을 더해준 것이죠. ${G}$의 입장을 생각해봅시다. ${G}$는 위의 가치함수를 최소화해야합니다. 따라서 뒤의 항, ${I(c; G(z, c))}$을 최대화해야합니다. 이는 latent code ${c}$와 Generator가 만들어내는 샘플들의 분포 ${G(z, c)}$가 높은 상호정보량을 가져야함을 의미하는 것이죠.

하지만 이 과정에서 ${P(c \vert x)}$를 알아야만 위의 상호정보량을 구할 수 있다는 문제가 발생합니다. 이에 우리는 어떠한 분포 ${Q(c\vert x)}$를 이용해 ${L_I (G, Q)}$를 ${I(c; G(z, c))}$이 lower bound, 하한선이 되게 합니다. 상호정보량을 직접 구하는 대신 ${Q}$를 사용해 하한선을 표현하고 이 하한선을 최대화하는 것이죠. 따라서 Loss를 재정의하면

$${\min_{G \cdot Q} \max_{D} V_{InfoGAN}(D, G, Q) = V(D, G) - \lambda L_I(G, Q)}$$

가 됩니다.

## Implementation

실제로 구현하는 과정에서는 ${Q}$를 신경망을 통해 만들어냅니다. 기존의 GAN과 비교했을 때 학습에 필요한 시간이 더 오래걸리지 않을까 걱정하시는 분들이 계신다면 전혀 걱정할 필요가 없습니다. ${D}$와 ${Q}$는 대부분의 네트워크를 공유하기 때문에 기존의 GAN과 비교했을 때 무시할 수 있는 수준의 연산이 늘어나는 정도입니다.

그래서 이 논문의 구현결과는 어땠을까요? 구현 과정에서 네트워크의 구조는 DCGAN에서 주장한 테크닉들을 그대로 사용했다고 합니다.

![mnist result](https://user-images.githubusercontent.com/25279765/29565083-740a1446-877f-11e7-9544-a8d284cdce66.png)

${c_1}$은 0~9를 뽑을 확률이 각각 0.1인 이산 확률 분포이며, ${c_2}$와 ${c_3}$은 -1 ~ 1사이의 값을 가지는 uniform disctribution입니다. 결과를 보면 ${c_1}$을 바꿔주면 숫자가 바뀌게 되고 ${c_2}$와 ${c_3}$이 각각 각도와 너비를 변화시켜주는 것을 확인할 수 있습니다. 위의 사진을 보면 ${c_2}$와 ${c_3}$을 변화시켰을 때,  단순히 원래 데이터에서 너비나 각도가 바뀌는 것이 아니라 진짜 데이터 같게 약간씩 스타일이 달라지는 것을 보여줍니다. 또한 주목할 만한 것은 -1 ~ 1사이에서 뽑은 값들로 학습을 시킨 것임에도 불구하고 -2 ~ 2의 범위에도 대응할 수 있다는 것입니다. 이러한 부분들에서 데이터가 잘 펼쳐져 있다고 판단할 수 있겠죠.

이외의 다른 데이터셋에 대해서도 좋은 결과를 얻어냈습니다.

![face result](https://user-images.githubusercontent.com/25279765/29565275-3ca4d044-8780-11e7-8814-57edbc233e37.png)

![chair result](https://user-images.githubusercontent.com/25279765/29565281-4037c784-8780-11e7-8e8d-7e972599432c.png)

## Reference

상호 정보량: [http://shineware.tistory.com/entry/%EC%83%81%ED%98%B8%EC%A0%95%EB%B3%B4%EB%9F%89Mutual-Information](http://shineware.tistory.com/entry/%EC%83%81%ED%98%B8%EC%A0%95%EB%B3%B4%EB%9F%89Mutual-Information)

InfoGAN 논문: [https://arxiv.org/pdf/1606.03657.pdf](https://arxiv.org/pdf/1606.03657.pdf)
