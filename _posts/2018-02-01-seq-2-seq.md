---
layout: post
title: Sequence to Sequence Learning with Neural Networks
excerpt: "Seq2seq 논문을 읽고 정리해봤습니다."
category: ML
---

## Seq2seq 리뷰

오늘 소개할 논문은 [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)라는 논문입니다. 우리가 일반적으로 생각하는 Deep Neural Network의 경우에는 입력의 크기가 정해져있습니다. 물론 Convolution 연산의 경우에는 입력의 크기에서 자유로운 점이 있긴하지만, Multi-Layer Perceptron의 경우를 생각해본다면 입력의 차원과 출력의 차원이 이미 정의된 상태에서 학습을 시작하죠. 이러한 방식은 크기가 고정된 이미지와 같은 데이터를 사용해야합니다. 그런데 우리가 말하는 말, 혹은 번역같은 것을 생각해보면 입력과 출력이 고정되어 있지 않습니다. 어떤 때에는 짧은 문장이 들어올 것이고, 또 어떤 때에는 엄청 긴 문장이 들어올 수 있겠죠. 이런 상황에서는 학습을 제대로 시킬 수가 없을 겁니다. 이를 해결하기 위해 나온 방법이 Long Short-Term Mermory, LSTM를 사용한 seq2seq입니다.

## Intro

이 모델의 가장 큰 특징은 위에서도 언급했듯, 입력과 출력의 크기가 고정되지 않아도 학습이 가능하다는 점입니다. 또한 Encoder와 Decoder, 2개의 LSTM을 둡니다. 인코더의 경우에는 입력 문장을 받아서 고정된 크기의 벡터로 표현해주고, Decoder는 이 벡터를 사용해 다시 다른 문장을 만들어내는 것이죠.

![image](https://user-images.githubusercontent.com/25279765/35659962-98278ff0-074c-11e8-98df-c7b223983da1.png)
> 입력 ABC를 받아 출력 WXYZ를 만들어내는 그림. EOS는 End-Of-Sentence로 문장이 끝났음을 나타내는 말이다.

이러한 방식을 사용한 결과 엄청 긴 문장에서도 괜찮은 성능을 보여줄 수 있다고 합니다.

일반적인 RNN의 경우에는 깊어질수록 학습이 잘되지 않는 문제를 보여주는데요, LSTM을 사용하면 이러한 문제에서 어느정도 벗어날 수가 있습니다. LSTM이 결국 예측하고자 하는 것은 조건부 확률 $p(y_1, ..., y_{T'} \lvert x_1,...,x_T)$입니다. $(x_1, ...,x_T)$는 입력을 의미하고, $(y_1,...y_{T'})$은 출력을 의미하는데요, $T$와 $T'$이 다를 수 있다는 점이 특징입니다. 이때 위에서 언급했듯이 입력을 받아 고정된 형태의 벡터, $v$를 만들고(LSTM의 hidden state를 사용합니다) 이 정보를 활용해 출력을 만들어냅니다. 이를 식으로 다시 표현하면

$$p(y_1,...,y_{T'} \lvert x_1,...x_T) = \prod_{t=1}^{T'}{p(y_t \lvert v, y_1, ..., y_{t-1})}$$

이 되는데요, 우변의 $p(y_t \lvert v, y_1, ..., y_{t-1})$는 그냥 vocabulary 내의 모든 단어에 softmax를 적용한 것입니다.

이때 논문에서는 몇가지 변화를 줬습니다

1. 2개의 LSTM을 사용했습니다. 즉 Encoder와 Decoder 네트워크를 분리한 것이죠. 이렇게 하면 모델의 파라미터 수가 늘어나고 더 많은 양의 학습이 가능하기 떄문이라고 합니다.
2. 깊은 LSTM이 얕은 모델보다 더 좋은 성능을 보였습니다. 따라서 4개의 layer를 사용했습니다.
3. 학습과정에서 입력의 순서를 뒤집었습니다. abc이면 cba순서로 입력을 바꿔준 것이죠. 실제 테스트할 때에는 이를 뒤집지 않았습니다. 이렇게 한 것이 더 좋은 성능이 나왔다고 하네요.

## Experiment

실험은 주로 영어-프랑스 문장 쌍들에 대해서 수행했습니다. 즉 번역을 수행하게 한 것인데요, source 언어, 즉 번역할 언어에서는 160,000개의 단어를 빈도순으로 사용했고 target 언어, 번역될 언어에서는 80,000개의 단어를 사용했다고 합니다. 만약 이 vocabulary에서 벗어날 경우에는 UNK라는 토큰을 추가해줬습니다.

학습의 전체적인 목표는 많은 문장쌍에 대해서 LSTM을 학습시키는 것입니다. 따라서 옳은 번역 $T$와 source sentence $S$가 있다고 했을 때, 우리는

$$1 / \left| \mathcal{S} \right| \sum_{(T, S) \in \mathcal{S}} \log p(T\lvert S)$$

log probability를 최대화하고자 합니다. 학습이 끝난 후 테스트에서는 가장 높은 가능성을 가진 단어를 찾아냅니다.

$$\hat{T} = \operatorname{argmax}p(T\lvert S)$$

이렇게 말이죠.

실제 예측 과정은 beam search를 사용했습니다. 적은 수 $B$만큼을 가능한 가설로 남겨두고, 입력이 들어오면서 이 가능한 가설들을 확장시켜나갑니다. 단 이렇게 하면 가능한 경우의 수가 너무 많아지므로, 가장 확률이 높은 $B$만 남겨두고 나머지는 버립니다.

이러한 방식이 문장의 길이에 robust하다는 장점을 갖지만 미니 배치안에 문장 길이가 다 제각각이라면 필요없는 연산이 많아집니다. 따라서 미니 배치안에는 비슷한 길이의 문장이 모이도록 해줬습니다. 그래서 논문에서 사용한 모델은 4 layer LSTM이며 각 레이어에는 1000개의 cell이 있습니다. 단어 임베딩에는 1000차원을 사용했구요.

![screenshot 2018-02-01 at 13 02 23](https://user-images.githubusercontent.com/25279765/35660574-29684b96-0750-11e8-9409-6502ff4f26d6.jpg)

이러한 방식의 특징은 문장을 인코딩해서 하나의 벡터로 표현할 수 있다는 것이죠. 이에 대해 PCA를 수행한 결과입니다. 문장의 어순에 따라 다른 값들을 가진다는 것을 확인하실 수 있습니다.

이외에도 다른 실험들을 수행했는데요, 그냥 좋은 결과가 나왔다의 이야기라 생략하겠습니다.
