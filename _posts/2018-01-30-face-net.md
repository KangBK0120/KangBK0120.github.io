---
layout: post
title: FaceNet - A unified Embedding for Face Recognition and Clustering 리뷰
excerpt: "Face Recognition/Verification을 위한 CNN 기반의 모델 FaceNet에 대한 리뷰입니다."
categories: [CNN, Review]
comments: true
use_math: true
---

# FaceNet 리뷰
<h6 align="right">강병규</h6>

오늘 리뷰할 논문은 [FaceNet: A unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832)입니다. 흔히 생각하는 Face Recognition에 관한 논문인데요. 독특한 방식을 통해서 단순히 거리만으로 유사도를 표현할 수 있게 한 논문입니다.

## Intro

얼굴과 관련된 컴퓨터비전 문제는 크게 Verification(같은 사람인가?), Recognition(이 사람이 어누구인가?), Clustering(유사한 얼굴 찾기) 정도가 있습니다. 이 논문의 경우에는 이 세가지 모두를 어느정도 처리할 수 있다는 특징을 가집니다. 기본적으로 Convolutional Neural Network를 사용하지만 우리가 흔히 생각하는 Softmax를 이용하는 구조와는 조금 차이가 있습니다.

이전의 경우에는 일반적으로 생각하는 CNN의 구조를 사용하면서 중간 단계의 있는 레이어의 출력값을 사용했습니다. 생각해보면 이는 조금 비효율적입니다. 새로운 얼굴에 대해 일반화가 잘 될지도 보장할 수 없을 뿐더러, 중간 단계의 출력을 사용하기 때문에 이 값의 차원이 매우 큽니다. 물론 PCA같은 방법을 사용할 수도 있겠지만 이는 그저 FC 레이어를 하나 더 쌓는 것과 다르지 않죠.

![screenshot 2018-01-31 at 02 00 11](https://user-images.githubusercontent.com/25279765/35579708-9dc463fc-062a-11e8-9ac3-6a3b9e78e5be.jpg)

여기서 구현한 네트워크는 저차원의 embedding을 적절히 구현하는 것에 있습니다. 그것도 중간 단계의 출력을 사용하는 것이 아닌, direct하게 말이죠. 이때 단순히 고차원의 얼굴 데이터를 저차원으로 압축하는 것이 아니라, 저차원으로 압축된 데이터 간의 거리로 유사도를 표현할 수 있게 했습니다. 우리가 흔히 생각하는 Euclidean 거리로 말입니다. 즉 임베딩한 거리가 가까울수록 유사한 얼굴이 되는 것이죠. 이렇게 되면 Verification문제는 그저 두 임베딩한 벡터 사이의 거리가 특정 역치보다 큰지 작은지 문제로 바꿀 수 있고, Recognition문제의 경우에는 KNN과 동일한 문제가 됩니다. 마지막으로 Clustering의 경우에는 우리가 흔히 생각하는 k-means 알고리즘 등을 적용하기만 되죠. 또한 이 방식을 사용하면 학습 데이터에 대한 특별한 처리를 해줄 필요가 없습니다. 그저 얼굴에 맞게 crop만 해주면 된다는 장점을 갖죠.

자 그래서 어떻게 저차원의 벡터를 학습을 시킬까요?

## Triplet Loss

이를 위해서 논문에서는 Triplet loss라는 방법을 제안합니다. 우리는 벡터 사이의 거리가 유사도가 되게 하고 싶습니다. 동일인의 사진은 가까운 거리에, 다른 사람의 사진은 먼 거리에 있도록 말이죠. 이를 loss로 표현해주면 됩니다. 기준이 되는 Anchor Image, Anchor와 동일 인물의 데이터인 Positive, Anchor와 다른 사람의 사진인 Negative. 총 세 개의 사진을 사용하기 때문에 Triplet Loss라고 부릅니다.

![screenshot 2018-01-31 at 02 00 23](https://user-images.githubusercontent.com/25279765/35579709-9df74cc2-062a-11e8-85e8-1f9579a91ab2.jpg)

우리가 만들어내는 임베딩을 $f(x)$라고 표현하고 Anchor는 $x^a$, Positive는 $x^p$, Negative는 $x^n$이라고 해봅시다. 이때 여기서 $\lVert f(x) \rVert_2 = 1$을 만족하도록 해줍니다. 즉 모든 임베딩들이 원점으로부터 같은 거리에 있는 것이죠.

자 동일인의 사진은 가깝게, 다른 사람의 사진은 멀게. 이를 식으로 표현하면 이렇게 됩니다.

$$\lVert f(x^a_i) - f(x^p_i) \rVert^2_2 + \alpha < \lVert f(x^a_i) - f(x^n_i) \rVert^2_2$$

여기서 $\alpha$는 positive와 negative 사이에 주고 싶은 margin을 의미한다고 생각하면 됩니다. Loss는 그냥 저 식을 합해주기만 하면 됩니다.

$$\sum^N_i{ (\lVert f(x^a_i) - f(x^p_i) \rVert^2_2 - \lVert f(x^a_i) - f(x^n_i) \rVert^2_2+ \alpha  )}$$

이렇게 말이죠.

근데 말이죠, 모든 가능한 triplet을 만들면서 학습을 진행한다고 해봅시다. 아마 위의 식을 너무 쉽게 만족하는 경우가 많겠죠. 완전 다른 사람의 사진이라면 말입니다. 이렇게 되면 학습이 제대로 되지 않을 것입니다. 결국 중요한 것은 **(1)을 만족하지 않는 triplet** 을 만들어야합니다. 잘 구분하지 못하는 사진을 넣어 줘야하는 거죠. 잘 맞는 건 크게 신경 쓸 필요가 없습니다. 중요한 건 많이 틀리는 걸 맞게하는 거겠죠. 그래서 주구장창 그런 문제만 푸는 것과 유사하다고 생각하시면 됩니다.

그러니까 최대한 먼 positive, hard-positive를 고르고($\operatorname{argmax}_{x^p_i} \lVert f(x^a_i) - f(x^p_i) \rVert^2_2$ ), 반대로 최대한 가까운 negative, hard-negative를 골라야 합니다($\operatorname{argmin}_{x^p_i} \lVert f(x^a_i) - f(x^n_i) \rVert^2_2$).  하지만 전체 데이터에서 이런 데이터들을 계속 찾아야한다고 생각하면 너무 비효율적이고 시간이 많이 필요합니다. 대신 여기서는 mini-batch에서 이런 데이터를 찾도록 했습니다. 이를 위해서 엄청나게 큰 배치사이즈를 사용했고, 결국에는 CPU만을 사용할 수 있게 되었습니다.

또 hardest-positive를 찾는 대신 미니배치 안에 존재하는 모든 anchor-positive 쌍을 사용해서 학습을 시켰습니다. 이렇게 하는게 더 안정적이고 수렴이 빨랐다고 해요. hardest-negative는 계속해서 찾습니다. 하지만 학습 초기에 이 과정에서 bad minima에 빠질 수가 있습니다. hard-negative식을 보세요. 가장 쉽게 만족하는 경우가 언제일까요? 바로 $f(x) = 0$일 때 입니다. 어떤 사진이 들어오든 0을 출력한다면 모든 경우를 만족하게 되죠. 이를 해결하기 위해서

$$\lVert f(x^a_i) - f(x^p_i) \rVert^2_2 < \lVert f(x^a_i) - f(x^n_i) \rVert^2_2$$

를 만족하는 negative를 찾습니다. 이를 semi-hard라고 부르고요. 이렇게 뽑은 negative는 아마 위에서 우리가 설정한 margin $\alpha$ 안에 있는 데이터가 될 것입니다. 먼 거리를 갖도록 해줘야하니까 이러한 데이터만을 골라서 학습을 시킵니다.

## Evaluation

전반적인 학습 과정은 생략하도록 하겠습니다. 모델은 크게 2가지 형태를 사용했는데요, Inception을 사용한 모델과 Visualizaing and Understanding Convolutional Networks에서 사용한 모델을 사용했습니다. 이외에도 여러가지 조합을 실험했구요.

그래서 모델의 정확도를 어떻게 평가 할 수 있을까요? 여기서는 Verification에 대한 실험을 수행했는데요, $D(i, j)$는 두 데이터 $i$와 $j$의 squared L2 distance를 의미합니다. 그냥 Euclidean 거리의 제곱과 같죠. 같은 사람들의 사진 중 맞았다고 한 경우의 비율과, 다른 사람들의 사진 중 맞았다고 한 비율, 이 두 가지를 사용해 모델의 성능을 평가했습니다. 이렇게 한 결과 LFW 데이터에서 99.63%의 성능을 냈다고 합니다.

![screenshot 2018-01-31 at 02 02 13](https://user-images.githubusercontent.com/25279765/35579776-c4d01be4-062a-11e8-9338-9cb0d36bcb5a.jpg)

또 보시면 알겠지만 얼굴 방향이나 빛에도 변함없는 결과를 냈다고 합니다. 역치를 1.1로 잡는다면 모두 적절하게 구분하고 있다는 것을 확인할 수 있죠.
